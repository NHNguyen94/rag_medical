{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-27T07:32:55.107761Z",
     "start_time": "2025-05-27T07:32:38.807970Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/Users/nguyennguyen/Desktop/github_repos/personal/rag_medical/src/data/emotion_data/training_small.csv\")\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "# Simple tokenizer and vocab builder\n",
    "def build_vocab(texts):\n",
    "    tokens = [word for text in texts for word in text.split()]\n",
    "    vocab = {\"<pad>\": 0}\n",
    "    for word in tokens:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab):\n",
    "        self.data = [torch.tensor([vocab.get(word, 0) for word in text.split()]) for text in texts]\n",
    "        self.labels = torch.tensor(labels)\n",
    "    def __len__(self): return len(self.labels)\n",
    "    def __getitem__(self, idx): return self.data[idx], self.labels[idx]\n",
    "\n",
    "def collate(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    return pad_sequence(texts, batch_first=True), torch.tensor(labels)\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=128, num_classes=6):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        return self.fc(h_n[-1])\n",
    "\n",
    "# Prepare data\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(df[\"text\"], df[\"label\"], test_size=0.2)\n",
    "vocab = build_vocab(train_texts)\n",
    "train_ds = TextDataset(train_texts.tolist(), train_labels.tolist(), vocab)\n",
    "test_ds = TextDataset(test_texts.tolist(), test_labels.tolist(), vocab)\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate)\n",
    "test_dl = DataLoader(test_ds, batch_size=32, collate_fn=collate)\n",
    "\n",
    "# Train\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMClassifier(len(vocab)).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    for x, y in train_dl:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(model(x), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1} done\")\n",
    "\n",
    "# Evaluate\n",
    "model.eval()\n",
    "correct = total = 0\n",
    "with torch.no_grad():\n",
    "    for x, y in test_dl:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        preds = model(x).argmax(1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.size(0)\n",
    "print(f\"Accuracy: {correct / total:.2%}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 done\n",
      "Epoch 2 done\n",
      "Epoch 3 done\n",
      "Epoch 4 done\n",
      "Epoch 5 done\n",
      "Epoch 6 done\n",
      "Epoch 7 done\n",
      "Epoch 8 done\n",
      "Epoch 9 done\n",
      "Epoch 10 done\n",
      "Epoch 11 done\n",
      "Epoch 12 done\n",
      "Epoch 13 done\n",
      "Epoch 14 done\n",
      "Epoch 15 done\n",
      "Epoch 16 done\n",
      "Epoch 17 done\n",
      "Epoch 18 done\n",
      "Epoch 19 done\n",
      "Epoch 20 done\n",
      "Epoch 21 done\n",
      "Epoch 22 done\n",
      "Epoch 23 done\n",
      "Epoch 24 done\n",
      "Epoch 25 done\n",
      "Epoch 26 done\n",
      "Epoch 27 done\n",
      "Epoch 28 done\n",
      "Epoch 29 done\n",
      "Epoch 30 done\n",
      "Epoch 31 done\n",
      "Epoch 32 done\n",
      "Epoch 33 done\n",
      "Epoch 34 done\n",
      "Epoch 35 done\n",
      "Epoch 36 done\n",
      "Epoch 37 done\n",
      "Epoch 38 done\n",
      "Epoch 39 done\n",
      "Epoch 40 done\n",
      "Epoch 41 done\n",
      "Epoch 42 done\n",
      "Epoch 43 done\n",
      "Epoch 44 done\n",
      "Epoch 45 done\n",
      "Epoch 46 done\n",
      "Epoch 47 done\n",
      "Epoch 48 done\n",
      "Epoch 49 done\n",
      "Epoch 50 done\n",
      "Epoch 51 done\n",
      "Epoch 52 done\n",
      "Epoch 53 done\n",
      "Epoch 54 done\n",
      "Epoch 55 done\n",
      "Epoch 56 done\n",
      "Epoch 57 done\n",
      "Epoch 58 done\n",
      "Epoch 59 done\n",
      "Epoch 60 done\n",
      "Epoch 61 done\n",
      "Epoch 62 done\n",
      "Epoch 63 done\n",
      "Epoch 64 done\n",
      "Epoch 65 done\n",
      "Epoch 66 done\n",
      "Epoch 67 done\n",
      "Epoch 68 done\n",
      "Epoch 69 done\n",
      "Epoch 70 done\n",
      "Epoch 71 done\n",
      "Epoch 72 done\n",
      "Epoch 73 done\n",
      "Epoch 74 done\n",
      "Epoch 75 done\n",
      "Epoch 76 done\n",
      "Epoch 77 done\n",
      "Epoch 78 done\n",
      "Epoch 79 done\n",
      "Epoch 80 done\n",
      "Epoch 81 done\n",
      "Epoch 82 done\n",
      "Epoch 83 done\n",
      "Epoch 84 done\n",
      "Epoch 85 done\n",
      "Epoch 86 done\n",
      "Epoch 87 done\n",
      "Epoch 88 done\n",
      "Epoch 89 done\n",
      "Epoch 90 done\n",
      "Epoch 91 done\n",
      "Epoch 92 done\n",
      "Epoch 93 done\n",
      "Epoch 94 done\n",
      "Epoch 95 done\n",
      "Epoch 96 done\n",
      "Epoch 97 done\n",
      "Epoch 98 done\n",
      "Epoch 99 done\n",
      "Epoch 100 done\n",
      "Epoch 101 done\n",
      "Epoch 102 done\n",
      "Epoch 103 done\n",
      "Epoch 104 done\n",
      "Epoch 105 done\n",
      "Epoch 106 done\n",
      "Epoch 107 done\n",
      "Epoch 108 done\n",
      "Epoch 109 done\n",
      "Epoch 110 done\n",
      "Epoch 111 done\n",
      "Epoch 112 done\n",
      "Epoch 113 done\n",
      "Epoch 114 done\n",
      "Epoch 115 done\n",
      "Epoch 116 done\n",
      "Epoch 117 done\n",
      "Epoch 118 done\n",
      "Epoch 119 done\n",
      "Epoch 120 done\n",
      "Epoch 121 done\n",
      "Epoch 122 done\n",
      "Epoch 123 done\n",
      "Epoch 124 done\n",
      "Epoch 125 done\n",
      "Epoch 126 done\n",
      "Epoch 127 done\n",
      "Epoch 128 done\n",
      "Epoch 129 done\n",
      "Epoch 130 done\n",
      "Epoch 131 done\n",
      "Epoch 132 done\n",
      "Epoch 133 done\n",
      "Epoch 134 done\n",
      "Epoch 135 done\n",
      "Epoch 136 done\n",
      "Epoch 137 done\n",
      "Epoch 138 done\n",
      "Epoch 139 done\n",
      "Epoch 140 done\n",
      "Epoch 141 done\n",
      "Epoch 142 done\n",
      "Epoch 143 done\n",
      "Epoch 144 done\n",
      "Epoch 145 done\n",
      "Epoch 146 done\n",
      "Epoch 147 done\n",
      "Epoch 148 done\n",
      "Epoch 149 done\n",
      "Epoch 150 done\n",
      "Epoch 151 done\n",
      "Epoch 152 done\n",
      "Epoch 153 done\n",
      "Epoch 154 done\n",
      "Epoch 155 done\n",
      "Epoch 156 done\n",
      "Epoch 157 done\n",
      "Epoch 158 done\n",
      "Epoch 159 done\n",
      "Epoch 160 done\n",
      "Epoch 161 done\n",
      "Epoch 162 done\n",
      "Epoch 163 done\n",
      "Epoch 164 done\n",
      "Epoch 165 done\n",
      "Epoch 166 done\n",
      "Epoch 167 done\n",
      "Epoch 168 done\n",
      "Epoch 169 done\n",
      "Epoch 170 done\n",
      "Epoch 171 done\n",
      "Epoch 172 done\n",
      "Epoch 173 done\n",
      "Epoch 174 done\n",
      "Epoch 175 done\n",
      "Epoch 176 done\n",
      "Epoch 177 done\n",
      "Epoch 178 done\n",
      "Epoch 179 done\n",
      "Epoch 180 done\n",
      "Epoch 181 done\n",
      "Epoch 182 done\n",
      "Epoch 183 done\n",
      "Epoch 184 done\n",
      "Epoch 185 done\n",
      "Epoch 186 done\n",
      "Epoch 187 done\n",
      "Epoch 188 done\n",
      "Epoch 189 done\n",
      "Epoch 190 done\n",
      "Epoch 191 done\n",
      "Epoch 192 done\n",
      "Epoch 193 done\n",
      "Epoch 194 done\n",
      "Epoch 195 done\n",
      "Epoch 196 done\n",
      "Epoch 197 done\n",
      "Epoch 198 done\n",
      "Epoch 199 done\n",
      "Epoch 200 done\n",
      "Accuracy: 33.33%\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "94e3315463f43be4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
