{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "gsTD-Xe9rn3u",
    "ExecuteTime": {
     "end_time": "2025-06-21T14:20:34.873232Z",
     "start_time": "2025-06-21T14:20:34.837423Z"
    }
   },
   "source": [
    "from typing import List, Dict, Optional\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import torch\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from src.utils.enums import QuestionRecommendConfig"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T14:24:07.546956Z",
     "start_time": "2025-06-21T14:24:07.538754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class QuestionDataProcessor:\n",
    "    def __init__(\n",
    "            self,\n",
    "            data_dir: str = QuestionRecommendConfig.FINE_TUNE_DATA_DIR/\"CancerQA.csv\",\n",
    "            output_dir: str = QuestionRecommendConfig.FINE_TUNE_DATA_DIR,\n",
    "            embedding_dim: int = 768\n",
    "    ):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Initialize BERT model for embeddings\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # Create output directory\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def load_datasets(self) -> pd.DataFrame:\n",
    "        file_path = self.data_dir\n",
    "        print(f\"file_path: {file_path}\")\n",
    "        df = pd.read_csv(Path(file_path))\n",
    "        print(df.head())\n",
    "        df['source'] = Path(file_path).stem\n",
    "        return df\n",
    "\n",
    "    def preprocess_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Clean and preprocess the data.\"\"\"\n",
    "        df['cleaned_question'] = df['Question']\n",
    "        df = df.drop_duplicates(subset=['cleaned_question'])\n",
    "        df = df.dropna(subset=['cleaned_question'])\n",
    "\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(self.output_dir / \"cleaned_dataset.csv\", index=False)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def create_embeddings(self, questions: List[str]) -> List[float]:\n",
    "        \"\"\"Create embeddings for questions using BERT.\"\"\"\n",
    "        embeddings = []\n",
    "        for question in tqdm(questions, desc=\"Creating embeddings\"):\n",
    "            # Tokenize and create embedding\n",
    "            inputs = self.tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                # Use [CLS] token embedding\n",
    "                embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()[0].astype('float32')\n",
    "\n",
    "                # if len(embedding) != self.embedding_dim:\n",
    "                #     if len(embedding) < self.embedding_dim:\n",
    "                #         embedding = np.pad(embedding, (0, self.embedding_dim - len(embedding)))\n",
    "                #     else:\n",
    "                #         embedding = embedding[:self.embedding_dim]\n",
    "                if embedding.size != self.embedding_dim:\n",
    "                    raise ValueError(f\"Expected {self.embedding_dim}, got {emb.size}\")\n",
    "\n",
    "                embeddings.append(embedding)\n",
    "\n",
    "        return np.stack(embeddings, axis=0)\n",
    "\n",
    "    def build_faiss_index(self, questions: List[str], embeddings: List[List[float]]):\n",
    "        \"\"\"Build FAISS index for question retrieval.\"\"\"\n",
    "        embeddings_array = np.array(embeddings).astype('float32')\n",
    "\n",
    "        faiss_index = faiss.IndexFlatL2(self.embedding_dim)\n",
    "        faiss_index.add(embeddings_array)\n",
    "\n",
    "        return faiss_index\n",
    "\n",
    "\n",
    "    def process_datasets(self) -> Dict:\n",
    "        \"\"\"Process all datasets and prepare for training.\"\"\"\n",
    "        print(\"Loading datasets...\")\n",
    "        combined_df = self.load_datasets()\n",
    "\n",
    "        print(\"Preprocessing data...\")\n",
    "        processed_df = self.preprocess_data(combined_df)\n",
    "\n",
    "        print(\"Creating embeddings...\")\n",
    "        questions = processed_df['cleaned_question'].tolist()\n",
    "        embeddings = self.create_embeddings(questions)\n",
    "\n",
    "        print(\"Building FAISS index...\")\n",
    "        faiss_index = self.build_faiss_index(questions, embeddings)\n",
    "\n",
    "        # Save processed data\n",
    "        questions_mapping = {i: q for i, q in enumerate(questions)}\n",
    "        # Write out to disk\n",
    "        with open(f'{self.output_dir}/questions_mapping.json', \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(questions_mapping, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "        return {\n",
    "            'questions': questions,\n",
    "            'embeddings': embeddings,\n",
    "            'faiss_index': faiss_index,\n",
    "            'questions_mapping': questions_mapping,\n",
    "            'metadata': {\n",
    "                'num_questions': len(questions),\n",
    "                'embedding_dim': self.embedding_dim\n",
    "            }\n",
    "        }"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T14:20:40.531778Z",
     "start_time": "2025-06-21T14:20:40.527944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class QuestionGenerator:\n",
    "    def __init__(\n",
    "            self,\n",
    "            faiss_index,\n",
    "            questions_mapping: Dict[str, str],\n",
    "            output_dir: str = QuestionRecommendConfig.PROCESSED_DATA_DIR,\n",
    "            temperature: float = 0.7\n",
    "    ):\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.faiss_index = faiss_index\n",
    "        self.questions_mapping = questions_mapping\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def get_similar_questions(self, query_embedding, k: int = 5) -> List[str]:\n",
    "        \"\"\"Retrieve similar questions using FAISS.\"\"\"\n",
    "        if isinstance(query_embedding, list):\n",
    "            query_embedding = np.array(query_embedding)\n",
    "\n",
    "        if query_embedding.ndim == 1:\n",
    "            query_embedding = query_embedding.reshape(1, -1)\n",
    "\n",
    "        distances, indices = self.faiss_index.search(query_embedding.astype('float32'), k)\n",
    "        # Get questions from mapping\n",
    "        updated_indices = indices[0][1:]\n",
    "        similar_questions = [self.questions_mapping[idx] for idx in updated_indices\n",
    "                           if idx in self.questions_mapping]\n",
    "\n",
    "        # Remove duplicates while preserving order\n",
    "        results = []\n",
    "        for q in similar_questions:\n",
    "            if q not in results:\n",
    "                results.append(q)\n",
    "\n",
    "        return results[:k]\n",
    "\n",
    "    def generate_follow_up_questions(\n",
    "            self,\n",
    "            question_embedding: np.ndarray,\n",
    "            num_questions: int = 4\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Generate follow-up questions using FAISS.\"\"\"\n",
    "        similar_questions = self.get_similar_questions(question_embedding, k=num_questions)\n",
    "        return similar_questions[:num_questions]"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T14:20:45.694100Z",
     "start_time": "2025-06-21T14:20:45.686454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "question_map_path = '/home/jiso/Documents/EPITA/action-learning/rag_medical/src/data/processed/questions_mapping.json'\n",
    "with open (question_map_path, 'r', encoding='utf-8') as f:\n",
    "    question_map = json.load(f)\n",
    "\n",
    "print(question_map[str(600)])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many people are affected by congenital hepatic fibrosis ?\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T14:20:47.202412Z",
     "start_time": "2025-06-21T14:20:47.196699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
    "\n",
    "@dataclass\n",
    "class CustomDataCollatorForSeq2Seq:\n",
    "    \"\"\"\n",
    "    Custom data collator that handles T5 input/output format properly\n",
    "    \"\"\"\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    model: Any = None\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: int = None\n",
    "    pad_to_multiple_of: int = None\n",
    "    label_pad_token_id: int = -100\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        # Separate inputs and labels\n",
    "        input_ids = [f[\"input_ids\"] for f in features]\n",
    "        attention_mask = [f[\"attention_mask\"] for f in features]\n",
    "        labels = [f[\"labels\"] for f in features]\n",
    "\n",
    "        # Convert to tensors\n",
    "        batch = {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "        return batch"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T14:24:14.942687Z",
     "start_time": "2025-06-21T14:24:14.924923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "class FineTuningPipeline:\n",
    "    def __init__(\n",
    "            self,\n",
    "            model_name: str = \"google/flan-t5-base\",\n",
    "            data_dir: str = QuestionRecommendConfig.FINE_TUNE_DATA_DIR,\n",
    "            output_dir: str = QuestionRecommendConfig.MODEL_DIR,\n",
    "            max_length: int = 256,\n",
    "            batch_size: int = 2,\n",
    "            learning_rate: float = 2e-5,\n",
    "            num_epochs: int = 3\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "        # Initialize components\n",
    "        self.data_processor = QuestionDataProcessor(data_dir=data_dir)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # Create output directory\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def prepare_training_data(self) -> Dict[str, Dataset]:\n",
    "        \"\"\"Prepare the dataset for training.\"\"\"\n",
    "        # Process datasets\n",
    "        processed_data = self.data_processor.process_datasets()\n",
    "\n",
    "        # Initialize question generator\n",
    "        question_generator = QuestionGenerator(\n",
    "            faiss_index=processed_data['faiss_index'],\n",
    "            questions_mapping=processed_data['questions_mapping']\n",
    "        )\n",
    "\n",
    "        # Generate training pairs\n",
    "        training_data = []\n",
    "        for idx, question in tqdm(enumerate(processed_data['questions']),\n",
    "                                 total=len(processed_data['questions']),\n",
    "                                 desc=\"Generating training data\"):\n",
    "            question_embedding = processed_data['embeddings'][idx]\n",
    "\n",
    "            follow_up_questions = question_generator.generate_follow_up_questions(\n",
    "                question_embedding,\n",
    "                num_questions=4\n",
    "            )\n",
    "\n",
    "            training_data.append({\n",
    "                'input': question,\n",
    "                'output': follow_up_questions,\n",
    "                'follow_up_combined': ' | '.join(follow_up_questions)\n",
    "            })\n",
    "\n",
    "        print(f\"Sample training data:\")\n",
    "        for i in range(min(3, len(training_data))):\n",
    "            print(f\"Input: {training_data[i]['input']}\")\n",
    "            print(f\"Output: {training_data[i]['output']}\")\n",
    "            print(\"---\")\n",
    "\n",
    "        # Convert to DataFrame for inspection\n",
    "        df = pd.DataFrame(training_data)\n",
    "\n",
    "        # Save to CSV for manual inspection\n",
    "        csv_path = self.output_dir / \"training_data.csv\"\n",
    "        df.to_csv(csv_path, index=False)\n",
    "\n",
    "        # Convert to dataset\n",
    "        dataset = Dataset.from_list(training_data)\n",
    "        split_dataset = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "        # Tokenize both splits\n",
    "        tokenized_dataset = {\n",
    "            'train': split_dataset['train'].map(\n",
    "                self.tokenize_function,\n",
    "                batched=True,\n",
    "                remove_columns=dataset.column_names\n",
    "            ),\n",
    "            'validation': split_dataset['test'].map(\n",
    "                self.tokenize_function,\n",
    "                batched=True,\n",
    "                remove_columns=dataset.column_names\n",
    "            )\n",
    "        }\n",
    "\n",
    "        print(\"Sample tokenized data:\")\n",
    "        sample = tokenized_dataset['train'][0]\n",
    "        print(f\"Input IDs shape: {len(sample['input_ids'])}\")\n",
    "        print(f\"Labels shape: {len(sample['labels'])}\")\n",
    "        print(f\"First few input IDs: {sample['input_ids'][:10]}\")\n",
    "        print(f\"First few labels: {sample['labels'][:10]}\")\n",
    "\n",
    "        labels_flat = [label for labels in tokenized_dataset['train']['labels'] for label in labels]\n",
    "        num_ignored = sum(1 for label in labels_flat if label == -100)\n",
    "        print(f\"Number of ignored tokens (-100): {num_ignored}/{len(labels_flat)}\")\n",
    "\n",
    "        return tokenized_dataset\n",
    "\n",
    "    def tokenize_function(self, examples):\n",
    "        \"\"\"Tokenize the input and output sequences.\"\"\"\n",
    "        model_inputs = self.tokenizer(\n",
    "            examples[\"input\"],\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        # Join the list of questions with a separator\n",
    "        formatted_outputs = [\" | \".join(questions) for questions in examples[\"output\"]]\n",
    "\n",
    "        # Tokenize targets\n",
    "        # with self.tokenizer.as_target_tokenizer():\n",
    "        #     labels = self.tokenizer(\n",
    "        #         formatted_outputs,\n",
    "        #         max_length=self.max_length,\n",
    "        #         padding=\"max_length\",\n",
    "        #         truncation=True\n",
    "        #     )\n",
    "\n",
    "        labels = self.tokenizer(\n",
    "            text_target=formatted_outputs,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        labels_input_ids = labels[\"input_ids\"].copy()\n",
    "        labels_input_ids = [\n",
    "            [(label if label != self.tokenizer.pad_token_id else -100) for label in label_seq]\n",
    "            for label_seq in labels_input_ids\n",
    "        ]\n",
    "\n",
    "        model_inputs[\"labels\"] = labels_input_ids\n",
    "        return model_inputs\n",
    "\n",
    "    def debug_batch(self, datasets):\n",
    "        \"\"\"Debug what's actually being fed to the model\"\"\"\n",
    "        from torch.utils.data import DataLoader\n",
    "\n",
    "        # Create a simple data loader\n",
    "        train_loader = DataLoader(datasets['train'], batch_size=2, collate_fn=lambda x: x)\n",
    "\n",
    "        # Get one batch\n",
    "        raw_batch = next(iter(train_loader))\n",
    "        print(\"Raw batch structure:\")\n",
    "        for i, item in enumerate(raw_batch[:2]):  # First 2 items\n",
    "            print(f\"Item {i}:\")\n",
    "            print(f\"  Input IDs length: {len(item['input_ids'])}\")\n",
    "            print(f\"  Labels length: {len(item['labels'])}\")\n",
    "            print(f\"  Input IDs sample: {item['input_ids'][:10]}\")\n",
    "            print(f\"  Labels sample: {item['labels'][:10]}\")\n",
    "            print(f\"  Labels has -100: {-100 in item['labels']}\")\n",
    "            print()\n",
    "\n",
    "        # Test with custom collator\n",
    "        data_collator = CustomDataCollatorForSeq2Seq(\n",
    "            tokenizer=self.tokenizer,\n",
    "            model=self.model,\n",
    "            label_pad_token_id=-100\n",
    "        )\n",
    "\n",
    "        collated_batch = data_collator(raw_batch)\n",
    "        print(\"Collated batch:\")\n",
    "        print(f\"Input IDs shape: {collated_batch['input_ids'].shape}\")\n",
    "        print(f\"Labels shape: {collated_batch['labels'].shape}\")\n",
    "        print(f\"Labels contains -100: {(collated_batch['labels'] == -100).any().item()}\")\n",
    "\n",
    "        # Test forward pass with this batch\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                # Move to device\n",
    "                batch_device = {k: v.to(self.device) for k, v in collated_batch.items()}\n",
    "                outputs = self.model(**batch_device)\n",
    "                print(f\"Forward pass loss: {outputs.loss.item()}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Forward pass failed: {e}\")\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Train the model.\"\"\"\n",
    "        # Prepare dataset\n",
    "        datasets = self.prepare_training_data()\n",
    "\n",
    "        print(\"=== DEBUGGING BATCHES ===\")\n",
    "        self.debug_batch(datasets)\n",
    "        print(\"=== END DEBUGGING ===\")\n",
    "\n",
    "        # Training arguments\n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=str(self.output_dir),\n",
    "            # evaluation_strategy=\"epoch\",\n",
    "            eval_steps=100,\n",
    "            do_eval=True,\n",
    "            do_train=True,\n",
    "            save_steps=100,\n",
    "            eval_strategy=\"steps\",\n",
    "            save_strategy=\"steps\",\n",
    "            logging_strategy=\"steps\",\n",
    "            logging_steps=10,\n",
    "            learning_rate=self.learning_rate,\n",
    "            per_device_train_batch_size=self.batch_size,\n",
    "            per_device_eval_batch_size=self.batch_size,\n",
    "            num_train_epochs=self.num_epochs,\n",
    "            weight_decay=0.01,\n",
    "            save_total_limit=3,\n",
    "            predict_with_generate=False,\n",
    "            fp16=True,\n",
    "            logging_dir=str(self.output_dir / \"logs\"),\n",
    "            load_best_model_at_end=True,\n",
    "            log_level='info',\n",
    "            report_to=\"none\",\n",
    "            gradient_accumulation_steps=1,\n",
    "            max_grad_norm=1.0,\n",
    "            warmup_steps=100,\n",
    "            dataloader_pin_memory=False\n",
    "        )\n",
    "\n",
    "        # Data collator\n",
    "        data_collator = DataCollatorForSeq2Seq(\n",
    "            self.tokenizer,\n",
    "            model=self.model,\n",
    "            label_pad_token_id=-100\n",
    "        )\n",
    "\n",
    "        # Initialize trainer\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=datasets['train'],\n",
    "            eval_dataset=datasets['validation'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            data_collator=data_collator\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "\n",
    "        metrics = trainer.evaluate()\n",
    "        print(f\"Final validation loss: {metrics['eval_loss']:.4f}\")\n",
    "\n",
    "        # Save the model\n",
    "        trainer.save_model(str(self.output_dir))\n",
    "        self.tokenizer.save_pretrained(str(self.output_dir))"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T14:24:59.131725Z",
     "start_time": "2025-06-21T14:24:56.711466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pipeline = FineTuningPipeline(\n",
    "    model_name=\"google/flan-t5-base\",\n",
    "    data_dir=\"/home/jiso/Documents/EPITA/action-learning/rag_medical/src/data/fine_tune_dataset/CancerQA.csv\",\n",
    "    output_dir=\"/home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5\",\n",
    "    batch_size=4,\n",
    "    num_epochs=3\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T14:27:02.426584Z",
     "start_time": "2025-06-21T14:25:00.462538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train the model\n",
    "pipeline.train()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "file_path: /home/jiso/Documents/EPITA/action-learning/rag_medical/src/data/fine_tune_dataset/CancerQA.csv\n",
      "                                            Question  \\\n",
      "0         What is (are) Non-Small Cell Lung Cancer ?   \n",
      "1   Who is at risk for Non-Small Cell Lung Cancer? ?   \n",
      "2  What are the symptoms of Non-Small Cell Lung C...   \n",
      "3       How to diagnose Non-Small Cell Lung Cancer ?   \n",
      "4  What is the outlook for Non-Small Cell Lung Ca...   \n",
      "\n",
      "                                              Answer   topic  split  \n",
      "0  Key Points\\n                    - Non-small ce...  cancer  train  \n",
      "1  Smoking is the major risk factor for non-small...  cancer  train  \n",
      "2  Signs of non-small cell lung cancer include a ...  cancer   test  \n",
      "3  Tests that examine the lungs are used to detec...  cancer  train  \n",
      "4  Certain factors affect prognosis (chance of re...  cancer  train  \n",
      "Preprocessing data...\n",
      "Creating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating embeddings: 100%|██████████| 683/683 [00:03<00:00, 194.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating training data: 100%|██████████| 683/683 [00:00<00:00, 19296.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample training data:\n",
      "Input: What is (are) Non-Small Cell Lung Cancer ?\n",
      "Output: ['What is (are) Small Cell Lung Cancer ?', 'What is (are) Renal Cell Cancer ?', 'What is (are) Hypopharyngeal Cancer ?']\n",
      "---\n",
      "Input: Who is at risk for Non-Small Cell Lung Cancer? ?\n",
      "Output: ['Who is at risk for Endometrial Cancer? ?', 'Who is at risk for Small Cell Lung Cancer? ?', 'Who is at risk for Adult Primary Liver Cancer? ?']\n",
      "---\n",
      "Input: What are the symptoms of Non-Small Cell Lung Cancer ?\n",
      "Output: ['What are the symptoms of Small Cell Lung Cancer ?', 'What are the symptoms of Endometrial Cancer ?', 'What are the symptoms of Pancreatic Cancer ?']\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 614/614 [00:00<00:00, 1242.29 examples/s]\n",
      "Map: 100%|██████████| 69/69 [00:00<00:00, 1233.67 examples/s]\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "/tmp/ipykernel_104592/2872925217.py:239: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "/home/jiso/anaconda3/envs/al/lib/python3.11/site-packages/accelerate/accelerator.py:449: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "Using auto half precision backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized data:\n",
      "Input IDs shape: 256\n",
      "Labels shape: 256\n",
      "First few input IDs: [2645, 19, 44, 1020, 21, 30588, 11759, 332, 13159, 9422]\n",
      "First few labels: [2645, 19, 44, 1020, 21, 12318, 11759, 332, 13159, 9422]\n",
      "Number of ignored tokens (-100): 126671/157184\n",
      "=== DEBUGGING BATCHES ===\n",
      "Raw batch structure:\n",
      "Item 0:\n",
      "  Input IDs length: 256\n",
      "  Labels length: 256\n",
      "  Input IDs sample: [2645, 19, 44, 1020, 21, 30588, 11759, 332, 13159, 9422]\n",
      "  Labels sample: [2645, 19, 44, 1020, 21, 12318, 11759, 332, 13159, 9422]\n",
      "  Labels has -100: True\n",
      "\n",
      "Item 1:\n",
      "  Input IDs length: 256\n",
      "  Labels length: 256\n",
      "  Input IDs sample: [363, 33, 8, 5872, 21, 30588, 8505, 2935, 7419, 5744]\n",
      "  Labels sample: [363, 33, 8, 5872, 21, 30588, 2808, 22450, 1162, 2149]\n",
      "  Labels has -100: True\n",
      "\n",
      "Collated batch:\n",
      "Input IDs shape: torch.Size([2, 256])\n",
      "Labels shape: torch.Size([2, 256])\n",
      "Labels contains -100: True\n",
      "Forward pass loss: 1.9573472738265991\n",
      "=== END DEBUGGING ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 614\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 462\n",
      "  Number of trainable parameters = 247,577,856\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='462' max='462' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [462/462 01:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 69\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-100\n",
      "Configuration saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-100/config.json\n",
      "Configuration saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-100/generation_config.json\n",
      "Model weights saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-100/model.safetensors\n",
      "tokenizer config file saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-100] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 69\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-200\n",
      "Configuration saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-200/config.json\n",
      "Configuration saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-200/generation_config.json\n",
      "Model weights saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-200/model.safetensors\n",
      "tokenizer config file saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-200] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 69\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-300\n",
      "Configuration saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-300/config.json\n",
      "Configuration saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-300/generation_config.json\n",
      "Model weights saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-300/model.safetensors\n",
      "tokenizer config file saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-300] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 69\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-400\n",
      "Configuration saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-400/config.json\n",
      "Configuration saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-400/generation_config.json\n",
      "Model weights saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-400/model.safetensors\n",
      "tokenizer config file saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-400] due to args.save_total_limit\n",
      "Saving model checkpoint to /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-462\n",
      "Configuration saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-462/config.json\n",
      "Configuration saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-462/generation_config.json\n",
      "Model weights saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-462/model.safetensors\n",
      "tokenizer config file saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-462/tokenizer_config.json\n",
      "Special tokens file saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-462/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 69\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:00]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5\n",
      "Configuration saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/config.json\n",
      "Configuration saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final validation loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/model.safetensors\n",
      "tokenizer config file saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/tokenizer_config.json\n",
      "Special tokens file saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/special_tokens_map.json\n",
      "tokenizer config file saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/tokenizer_config.json\n",
      "Special tokens file saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/special_tokens_map.json\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T14:43:43.705085Z",
     "start_time": "2025-06-21T14:43:43.048952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_recommendations(model, tokenizer, question: str, max_length: int = 256) -> List[str]:\n",
    "    \"\"\"Generate question recommendations for a given medical question.\"\"\"\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        question,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    # Generate recommendations\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        num_beams=4,\n",
    "        temperature=0.7,\n",
    "        no_repeat_ngram_size=2\n",
    "    )\n",
    "\n",
    "    # Decode and split recommendations\n",
    "    recommendations = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return recommendations.split(\" | \")\n",
    "\n",
    "question = \"What causes cancer?\"\n",
    "# Example usage\n",
    "prompt = (\n",
    "            \"Generate exactly four medically relevant follow-up questions based on the patient’s input question.\"\n",
    "            \"Each follow-up question must be concise, end with a question mark, and explore a different aspect of the topic \"\n",
    "            \"(e.g., diagnosis, treatment, risk factors, prognosis, prevention, or causes). \"\n",
    "            \"The follow-up questions must not repeat the patient’s question or use its exact wording. \"\n",
    "            \"Format the output as a numbered list (e.g., '1. Question?\\n2. Question?\\n3. Question?\\n4. Question?').\\n\\n\"\n",
    "            \"Example 1:\\n\"\n",
    "            \"Patient: \\\"What are the symptoms of diabetes?\\\"\\n\"\n",
    "            \"Follow-Up Questions:\\n\"\n",
    "            \"1. How is diabetes diagnosed?\\n\"\n",
    "            \"2. What are the treatment options for diabetes?\\n\"\n",
    "            \"3. Who is at risk for developing diabetes?\\n\"\n",
    "            \"4. What complications can arise from diabetes?\\n\\n\"\n",
    "            \"Example 2:\\n\"\n",
    "            \"Patient: \\\"What is breast cancer?\\\"\\n\"\n",
    "            \"Follow-Up Questions:\\n\"\n",
    "            \"1. What are the main risk factors for breast cancer?\\n\"\n",
    "            \"2. How is breast cancer diagnosed?\\n\"\n",
    "            \"3. What treatments are available for breast cancer?\\n\"\n",
    "            \"4. What is the prognosis for breast cancer patients?\\n\\n\"\n",
    "            f\"Input Question: \\\"{question}\\\"\\n\"\n",
    "            \"Follow-Up Questions:\".format(question)\n",
    "        )\n",
    "recommendations = generate_recommendations(pipeline.model, pipeline.tokenizer, prompt)\n",
    "print(f\"Original question: {question}\")\n",
    "print(\"\\nRecommended follow-up questions:\")\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"{rec}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question: What causes cancer?\n",
      "\n",
      "Recommended follow-up questions:\n",
      "1. What causes cancer? 2. What is the main cause? 3. What are the causes? 4. What does it mean to be a cancer patient?\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T06:40:26.248808Z",
     "start_time": "2025-06-10T06:40:15.224040Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "# 1) Load metrics\n",
    "rouge_metric     = evaluate.load(\"rouge\")\n",
    "bleu_metric      = evaluate.load(\"bleu\")\n",
    "bertscore_metric = evaluate.load(\"bertscore\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    # 2) Decode predictions and labels\n",
    "    decoded_preds  = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # 3) Exact-match\n",
    "    pred_lists  = [p.split(\" | \") for p in decoded_preds]\n",
    "    label_lists = [l.split(\" | \") for l in decoded_labels]\n",
    "    em_scores   = [\n",
    "        sum(1 for p in pred if p in gold) / max(len(gold),1)\n",
    "        for pred, gold in zip(pred_lists, label_lists)\n",
    "    ]\n",
    "    exact_match = np.mean(em_scores)\n",
    "\n",
    "    # 4) ROUGE-L\n",
    "    rouge_preds  = [\"\\n\".join(p) for p in pred_lists]\n",
    "    rouge_labels = [\"\\n\".join(l) for l in label_lists]\n",
    "    rouge_res    = rouge_metric.compute(predictions=rouge_preds, references=rouge_labels)\n",
    "    rougeL_f1    = rouge_res[\"rougeL\"]  # already float\n",
    "\n",
    "    # 5) BLEU (string inputs)\n",
    "    # Provide list of hypothesis strings and list of reference strings\n",
    "    bleu_res  = bleu_metric.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels\n",
    "    )\n",
    "    bleu_score = bleu_res[\"bleu\"]\n",
    "\n",
    "    # 6) BERTScore\n",
    "    bert_res = bertscore_metric.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        lang=\"en\"\n",
    "    )\n",
    "    bert_f1 = np.mean(bert_res[\"f1\"])\n",
    "\n",
    "    return {\n",
    "        \"exact_match\": exact_match,\n",
    "        \"rougeL_f1\": rougeL_f1,\n",
    "        \"bleu\": bleu_score,\n",
    "        \"bertscore_f1\": bert_f1,\n",
    "    }\n",
    "\n",
    "# 7) Load your fine-tuned model & tokenizer\n",
    "model     = AutoModelForSeq2SeqLM.from_pretrained(pipeline.output_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pipeline.output_dir)\n",
    "\n",
    "# 8) Rebuild your eval_dataset from the saved CSV\n",
    "df = pd.read_csv(f\"{pipeline.output_dir}/training_data.csv\")\n",
    "val_df = df.sample(frac=0.1, random_state=42)\n",
    "eval_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "def preprocess(ex):\n",
    "    enc = tokenizer(\n",
    "        ex[\"input\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256\n",
    "    )\n",
    "    labs = tokenizer(\n",
    "        text_target=ex[\"follow_up_combined\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256\n",
    "    )[\"input_ids\"]\n",
    "    # mask pads\n",
    "    labs = [lab if lab != tokenizer.pad_token_id else -100 for lab in labs]\n",
    "    enc[\"labels\"] = labs\n",
    "    return enc\n",
    "\n",
    "eval_dataset = eval_dataset.map(preprocess, batched=False, remove_columns=eval_dataset.column_names)\n",
    "\n",
    "# 9) Create the Trainer and run evaluation\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=Seq2SeqTrainingArguments(\n",
    "        output_dir=str(pipeline.output_dir),\n",
    "        per_device_eval_batch_size=8,\n",
    "        predict_with_generate=True,\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model, label_pad_token_id=-100),\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset)\n",
    "print(\"Evaluation results:\", metrics)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/config.json\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n",
      "Map: 100%|██████████| 68/68 [00:00<00:00, 817.40 examples/s]\n",
      "PyTorch: setting up devices\n",
      "/tmp/ipykernel_5016/3773851284.py:94: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "/home/jiso/anaconda3/envs/al/lib/python3.11/site-packages/accelerate/accelerator.py:449: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 68\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:03]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/jiso/.cache/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /home/jiso/.cache/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/vocab.json\n",
      "loading file merges.txt from cache at /home/jiso/.cache/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/merges.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/jiso/.cache/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/tokenizer_config.json\n",
      "loading file tokenizer.json from cache at /home/jiso/.cache/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/tokenizer.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at /home/jiso/.cache/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/jiso/.cache/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/jiso/.cache/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/model.safetensors\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': nan, 'eval_model_preparation_time': 0.0038, 'eval_exact_match': 0.0, 'eval_rougeL_f1': 0.1179912991081328, 'eval_bleu': 0.002034812795709879, 'eval_bertscore_f1': 0.8239430899129194, 'eval_runtime': 6.5132, 'eval_samples_per_second': 10.44, 'eval_steps_per_second': 1.382}\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "c01b308a72d849299858056cb495ccc0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_11cf9f37c8d6466facaa35ce3afc8b2d",
       "IPY_MODEL_27320cdc61cf4f378ab136ca2b897ec6",
       "IPY_MODEL_e5d592822cb3468aa8841362f2ca5fcf"
      ],
      "layout": "IPY_MODEL_03bfae2d52384bcaa525619caa3120b2"
     }
    },
    "11cf9f37c8d6466facaa35ce3afc8b2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7a605609a84e4dec9cb3527887c5c293",
      "placeholder": "​",
      "style": "IPY_MODEL_d326f4cd660d4b95a68b42bc7b364e19",
      "value": "Map: 100%"
     }
    },
    "27320cdc61cf4f378ab136ca2b897ec6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0d2eadfe6d1344478eee6c9dfc712aab",
      "max": 614,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e63ea984e33c4d5e954697e55646888b",
      "value": 614
     }
    },
    "e5d592822cb3468aa8841362f2ca5fcf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f689bac961364670b99ec20f53e0cb83",
      "placeholder": "​",
      "style": "IPY_MODEL_64405c41e83645d1b55bad5adf3ec39a",
      "value": " 614/614 [00:00&lt;00:00, 3381.05 examples/s]"
     }
    },
    "03bfae2d52384bcaa525619caa3120b2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7a605609a84e4dec9cb3527887c5c293": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d326f4cd660d4b95a68b42bc7b364e19": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0d2eadfe6d1344478eee6c9dfc712aab": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e63ea984e33c4d5e954697e55646888b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f689bac961364670b99ec20f53e0cb83": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "64405c41e83645d1b55bad5adf3ec39a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e4f4e34a79f04ce9999622cfc0445b85": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ce9250c66d6c4512a90f6ce945b61eb1",
       "IPY_MODEL_e0061034ecce431cb21ddeb99c99f452",
       "IPY_MODEL_d46f2bbb3da24973960a6641dc927c7d"
      ],
      "layout": "IPY_MODEL_62ef645e1e78488cba14f2795b5e4613"
     }
    },
    "ce9250c66d6c4512a90f6ce945b61eb1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f64ab117b1294381aeba6457badea9c7",
      "placeholder": "​",
      "style": "IPY_MODEL_af773f2ccd6f430fa40ab175c96c9d37",
      "value": "Map: 100%"
     }
    },
    "e0061034ecce431cb21ddeb99c99f452": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c6c911728aba424c881eacaaf9429126",
      "max": 69,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2ee4740235ef4263bcbb11a4696f0f3b",
      "value": 69
     }
    },
    "d46f2bbb3da24973960a6641dc927c7d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_08249caf995544ff8b6abbd00aca78f8",
      "placeholder": "​",
      "style": "IPY_MODEL_a54c23f55e0e4be497625b68a6805375",
      "value": " 69/69 [00:00&lt;00:00, 1150.93 examples/s]"
     }
    },
    "62ef645e1e78488cba14f2795b5e4613": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f64ab117b1294381aeba6457badea9c7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af773f2ccd6f430fa40ab175c96c9d37": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c6c911728aba424c881eacaaf9429126": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2ee4740235ef4263bcbb11a4696f0f3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "08249caf995544ff8b6abbd00aca78f8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a54c23f55e0e4be497625b68a6805375": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
