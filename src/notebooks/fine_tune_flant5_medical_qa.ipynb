{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T14:20:34.873232Z",
     "start_time": "2025-06-21T14:20:34.837423Z"
    },
    "id": "gsTD-Xe9rn3u"
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import torch\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from src.utils.enums import QuestionRecommendConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T14:24:07.546956Z",
     "start_time": "2025-06-21T14:24:07.538754Z"
    }
   },
   "outputs": [],
   "source": [
    "class QuestionDataProcessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str = QuestionRecommendConfig.FINE_TUNE_DATA_DIR / \"CancerQA.csv\",\n",
    "        output_dir: str = QuestionRecommendConfig.FINE_TUNE_DATA_DIR,\n",
    "        embedding_dim: int = 768,\n",
    "    ):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Initialize BERT model for embeddings\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # Create output directory\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def load_datasets(self) -> pd.DataFrame:\n",
    "        file_path = self.data_dir\n",
    "        print(f\"file_path: {file_path}\")\n",
    "        df = pd.read_csv(Path(file_path))\n",
    "        print(df.head())\n",
    "        df[\"source\"] = Path(file_path).stem\n",
    "        return df\n",
    "\n",
    "    def preprocess_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Clean and preprocess the data.\"\"\"\n",
    "        df[\"cleaned_question\"] = df[\"Question\"]\n",
    "        df = df.drop_duplicates(subset=[\"cleaned_question\"])\n",
    "        df = df.dropna(subset=[\"cleaned_question\"])\n",
    "\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(self.output_dir / \"cleaned_dataset.csv\", index=False)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def create_embeddings(self, questions: List[str]) -> List[float]:\n",
    "        \"\"\"Create embeddings for questions using BERT.\"\"\"\n",
    "        embeddings = []\n",
    "        for question in tqdm(questions, desc=\"Creating embeddings\"):\n",
    "            # Tokenize and create embedding\n",
    "            inputs = self.tokenizer(\n",
    "                question, return_tensors=\"pt\", padding=True, truncation=True\n",
    "            )\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                # Use [CLS] token embedding\n",
    "                embedding = (\n",
    "                    outputs.last_hidden_state[:, 0, :]\n",
    "                    .cpu()\n",
    "                    .numpy()[0]\n",
    "                    .astype(\"float32\")\n",
    "                )\n",
    "\n",
    "                # if len(embedding) != self.embedding_dim:\n",
    "                #     if len(embedding) < self.embedding_dim:\n",
    "                #         embedding = np.pad(embedding, (0, self.embedding_dim - len(embedding)))\n",
    "                #     else:\n",
    "                #         embedding = embedding[:self.embedding_dim]\n",
    "                if embedding.size != self.embedding_dim:\n",
    "                    raise ValueError(f\"Expected {self.embedding_dim}, got {emb.size}\")\n",
    "\n",
    "                embeddings.append(embedding)\n",
    "\n",
    "        return np.stack(embeddings, axis=0)\n",
    "\n",
    "    def build_faiss_index(self, questions: List[str], embeddings: List[List[float]]):\n",
    "        \"\"\"Build FAISS index for question retrieval.\"\"\"\n",
    "        embeddings_array = np.array(embeddings).astype(\"float32\")\n",
    "\n",
    "        faiss_index = faiss.IndexFlatL2(self.embedding_dim)\n",
    "        faiss_index.add(embeddings_array)\n",
    "\n",
    "        return faiss_index\n",
    "\n",
    "    def process_datasets(self) -> Dict:\n",
    "        \"\"\"Process all datasets and prepare for training.\"\"\"\n",
    "        print(\"Loading datasets...\")\n",
    "        combined_df = self.load_datasets()\n",
    "\n",
    "        print(\"Preprocessing data...\")\n",
    "        processed_df = self.preprocess_data(combined_df)\n",
    "\n",
    "        print(\"Creating embeddings...\")\n",
    "        questions = processed_df[\"cleaned_question\"].tolist()\n",
    "        embeddings = self.create_embeddings(questions)\n",
    "\n",
    "        print(\"Building FAISS index...\")\n",
    "        faiss_index = self.build_faiss_index(questions, embeddings)\n",
    "\n",
    "        # Save processed data\n",
    "        questions_mapping = {i: q for i, q in enumerate(questions)}\n",
    "        # Write out to disk\n",
    "        with open(\n",
    "            f\"{self.output_dir}/questions_mapping.json\", \"w\", encoding=\"utf-8\"\n",
    "        ) as f:\n",
    "            json.dump(questions_mapping, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        return {\n",
    "            \"questions\": questions,\n",
    "            \"embeddings\": embeddings,\n",
    "            \"faiss_index\": faiss_index,\n",
    "            \"questions_mapping\": questions_mapping,\n",
    "            \"metadata\": {\n",
    "                \"num_questions\": len(questions),\n",
    "                \"embedding_dim\": self.embedding_dim,\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T14:20:40.531778Z",
     "start_time": "2025-06-21T14:20:40.527944Z"
    }
   },
   "outputs": [],
   "source": [
    "class QuestionGenerator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        faiss_index,\n",
    "        questions_mapping: Dict[str, str],\n",
    "        output_dir: str = QuestionRecommendConfig.PROCESSED_DATA_DIR,\n",
    "        temperature: float = 0.7,\n",
    "    ):\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.faiss_index = faiss_index\n",
    "        self.questions_mapping = questions_mapping\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def get_similar_questions(self, query_embedding, k: int = 5) -> List[str]:\n",
    "        \"\"\"Retrieve similar questions using FAISS.\"\"\"\n",
    "        if isinstance(query_embedding, list):\n",
    "            query_embedding = np.array(query_embedding)\n",
    "\n",
    "        if query_embedding.ndim == 1:\n",
    "            query_embedding = query_embedding.reshape(1, -1)\n",
    "\n",
    "        distances, indices = self.faiss_index.search(\n",
    "            query_embedding.astype(\"float32\"), k\n",
    "        )\n",
    "        # Get questions from mapping\n",
    "        updated_indices = indices[0][1:]\n",
    "        similar_questions = [\n",
    "            self.questions_mapping[idx]\n",
    "            for idx in updated_indices\n",
    "            if idx in self.questions_mapping\n",
    "        ]\n",
    "\n",
    "        # Remove duplicates while preserving order\n",
    "        results = []\n",
    "        for q in similar_questions:\n",
    "            if q not in results:\n",
    "                results.append(q)\n",
    "\n",
    "        return results[:k]\n",
    "\n",
    "    def generate_follow_up_questions(\n",
    "        self, question_embedding: np.ndarray, num_questions: int = 4\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Generate follow-up questions using FAISS.\"\"\"\n",
    "        similar_questions = self.get_similar_questions(\n",
    "            question_embedding, k=num_questions\n",
    "        )\n",
    "        return similar_questions[:num_questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T14:20:45.694100Z",
     "start_time": "2025-06-21T14:20:45.686454Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many people are affected by congenital hepatic fibrosis ?\n"
     ]
    }
   ],
   "source": [
    "question_map_path = \"/home/jiso/Documents/EPITA/action-learning/rag_medical/src/data/processed/questions_mapping.json\"\n",
    "with open(question_map_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    question_map = json.load(f)\n",
    "\n",
    "print(question_map[str(600)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T14:20:47.202412Z",
     "start_time": "2025-06-21T14:20:47.196699Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CustomDataCollatorForSeq2Seq:\n",
    "    \"\"\"\n",
    "    Custom data collator that handles T5 input/output format properly\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    model: Any = None\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: int = None\n",
    "    pad_to_multiple_of: int = None\n",
    "    label_pad_token_id: int = -100\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        # Separate inputs and labels\n",
    "        input_ids = [f[\"input_ids\"] for f in features]\n",
    "        attention_mask = [f[\"attention_mask\"] for f in features]\n",
    "        labels = [f[\"labels\"] for f in features]\n",
    "\n",
    "        # Convert to tensors\n",
    "        batch = {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T14:24:14.942687Z",
     "start_time": "2025-06-21T14:24:14.924923Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'QuestionRecommendConfig' has no attribute 'MODEL_DIR'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      2\u001b[39m     AutoModelForSeq2SeqLM,\n\u001b[32m      3\u001b[39m     AutoTokenizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m      6\u001b[39m     DataCollatorForSeq2Seq,\n\u001b[32m      7\u001b[39m )\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mFineTuningPipeline\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgoogle/flan-t5-base\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mFineTuningPipeline\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mFineTuningPipeline\u001b[39;00m:\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     13\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     14\u001b[39m         model_name: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mgoogle/flan-t5-base\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     15\u001b[39m         data_dir: \u001b[38;5;28mstr\u001b[39m = QuestionRecommendConfig.FINE_TUNE_DATA_DIR,\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m         output_dir: \u001b[38;5;28mstr\u001b[39m = \u001b[43mQuestionRecommendConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMODEL_DIR\u001b[49m,\n\u001b[32m     17\u001b[39m         max_length: \u001b[38;5;28mint\u001b[39m = \u001b[32m256\u001b[39m,\n\u001b[32m     18\u001b[39m         batch_size: \u001b[38;5;28mint\u001b[39m = \u001b[32m2\u001b[39m,\n\u001b[32m     19\u001b[39m         learning_rate: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m2e-5\u001b[39m,\n\u001b[32m     20\u001b[39m         num_epochs: \u001b[38;5;28mint\u001b[39m = \u001b[32m3\u001b[39m,\n\u001b[32m     21\u001b[39m     ):\n\u001b[32m     22\u001b[39m         \u001b[38;5;28mself\u001b[39m.model_name = model_name\n\u001b[32m     23\u001b[39m         \u001b[38;5;28mself\u001b[39m.data_dir = Path(data_dir)\n",
      "\u001b[31mAttributeError\u001b[39m: type object 'QuestionRecommendConfig' has no attribute 'MODEL_DIR'"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "class FineTuningPipeline:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"google/flan-t5-base\",\n",
    "        data_dir: str = QuestionRecommendConfig.FINE_TUNE_DATA_DIR,\n",
    "        output_dir: str = QuestionRecommendConfig.MODEL_DIR,\n",
    "        max_length: int = 256,\n",
    "        batch_size: int = 2,\n",
    "        learning_rate: float = 2e-5,\n",
    "        num_epochs: int = 3,\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "        # Initialize components\n",
    "        self.data_processor = QuestionDataProcessor(data_dir=data_dir)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # Create output directory\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def prepare_training_data(self) -> Dict[str, Dataset]:\n",
    "        \"\"\"Prepare the dataset for training.\"\"\"\n",
    "        # Process datasets\n",
    "        processed_data = self.data_processor.process_datasets()\n",
    "\n",
    "        # Initialize question generator\n",
    "        question_generator = QuestionGenerator(\n",
    "            faiss_index=processed_data[\"faiss_index\"],\n",
    "            questions_mapping=processed_data[\"questions_mapping\"],\n",
    "        )\n",
    "\n",
    "        # Generate training pairs\n",
    "        training_data = []\n",
    "        for idx, question in tqdm(\n",
    "            enumerate(processed_data[\"questions\"]),\n",
    "            total=len(processed_data[\"questions\"]),\n",
    "            desc=\"Generating training data\",\n",
    "        ):\n",
    "            question_embedding = processed_data[\"embeddings\"][idx]\n",
    "\n",
    "            follow_up_questions = question_generator.generate_follow_up_questions(\n",
    "                question_embedding, num_questions=4\n",
    "            )\n",
    "\n",
    "            training_data.append(\n",
    "                {\n",
    "                    \"input\": question,\n",
    "                    \"output\": follow_up_questions,\n",
    "                    \"follow_up_combined\": \" | \".join(follow_up_questions),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        print(f\"Sample training data:\")\n",
    "        for i in range(min(3, len(training_data))):\n",
    "            print(f\"Input: {training_data[i]['input']}\")\n",
    "            print(f\"Output: {training_data[i]['output']}\")\n",
    "            print(\"---\")\n",
    "\n",
    "        # Convert to DataFrame for inspection\n",
    "        df = pd.DataFrame(training_data)\n",
    "\n",
    "        # Save to CSV for manual inspection\n",
    "        csv_path = self.output_dir / \"training_data.csv\"\n",
    "        df.to_csv(csv_path, index=False)\n",
    "\n",
    "        # Convert to dataset\n",
    "        dataset = Dataset.from_list(training_data)\n",
    "        split_dataset = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "        # Tokenize both splits\n",
    "        tokenized_dataset = {\n",
    "            \"train\": split_dataset[\"train\"].map(\n",
    "                self.tokenize_function,\n",
    "                batched=True,\n",
    "                remove_columns=dataset.column_names,\n",
    "            ),\n",
    "            \"validation\": split_dataset[\"test\"].map(\n",
    "                self.tokenize_function,\n",
    "                batched=True,\n",
    "                remove_columns=dataset.column_names,\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        print(\"Sample tokenized data:\")\n",
    "        sample = tokenized_dataset[\"train\"][0]\n",
    "        print(f\"Input IDs shape: {len(sample['input_ids'])}\")\n",
    "        print(f\"Labels shape: {len(sample['labels'])}\")\n",
    "        print(f\"First few input IDs: {sample['input_ids'][:10]}\")\n",
    "        print(f\"First few labels: {sample['labels'][:10]}\")\n",
    "\n",
    "        labels_flat = [\n",
    "            label for labels in tokenized_dataset[\"train\"][\"labels\"] for label in labels\n",
    "        ]\n",
    "        num_ignored = sum(1 for label in labels_flat if label == -100)\n",
    "        print(f\"Number of ignored tokens (-100): {num_ignored}/{len(labels_flat)}\")\n",
    "\n",
    "        return tokenized_dataset\n",
    "\n",
    "    def tokenize_function(self, examples):\n",
    "        \"\"\"Tokenize the input and output sequences.\"\"\"\n",
    "        model_inputs = self.tokenizer(\n",
    "            examples[\"input\"],\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        # Join the list of questions with a separator\n",
    "        formatted_outputs = [\" | \".join(questions) for questions in examples[\"output\"]]\n",
    "\n",
    "        # Tokenize targets\n",
    "        # with self.tokenizer.as_target_tokenizer():\n",
    "        #     labels = self.tokenizer(\n",
    "        #         formatted_outputs,\n",
    "        #         max_length=self.max_length,\n",
    "        #         padding=\"max_length\",\n",
    "        #         truncation=True\n",
    "        #     )\n",
    "\n",
    "        labels = self.tokenizer(\n",
    "            text_target=formatted_outputs,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        labels_input_ids = labels[\"input_ids\"].copy()\n",
    "        labels_input_ids = [\n",
    "            [\n",
    "                (label if label != self.tokenizer.pad_token_id else -100)\n",
    "                for label in label_seq\n",
    "            ]\n",
    "            for label_seq in labels_input_ids\n",
    "        ]\n",
    "\n",
    "        model_inputs[\"labels\"] = labels_input_ids\n",
    "        return model_inputs\n",
    "\n",
    "    def debug_batch(self, datasets):\n",
    "        \"\"\"Debug what's actually being fed to the model\"\"\"\n",
    "        from torch.utils.data import DataLoader\n",
    "\n",
    "        # Create a simple data loader\n",
    "        train_loader = DataLoader(\n",
    "            datasets[\"train\"], batch_size=2, collate_fn=lambda x: x\n",
    "        )\n",
    "\n",
    "        # Get one batch\n",
    "        raw_batch = next(iter(train_loader))\n",
    "        print(\"Raw batch structure:\")\n",
    "        for i, item in enumerate(raw_batch[:2]):  # First 2 items\n",
    "            print(f\"Item {i}:\")\n",
    "            print(f\"  Input IDs length: {len(item['input_ids'])}\")\n",
    "            print(f\"  Labels length: {len(item['labels'])}\")\n",
    "            print(f\"  Input IDs sample: {item['input_ids'][:10]}\")\n",
    "            print(f\"  Labels sample: {item['labels'][:10]}\")\n",
    "            print(f\"  Labels has -100: {-100 in item['labels']}\")\n",
    "            print()\n",
    "\n",
    "        # Test with custom collator\n",
    "        data_collator = CustomDataCollatorForSeq2Seq(\n",
    "            tokenizer=self.tokenizer, model=self.model, label_pad_token_id=-100\n",
    "        )\n",
    "\n",
    "        collated_batch = data_collator(raw_batch)\n",
    "        print(\"Collated batch:\")\n",
    "        print(f\"Input IDs shape: {collated_batch['input_ids'].shape}\")\n",
    "        print(f\"Labels shape: {collated_batch['labels'].shape}\")\n",
    "        print(\n",
    "            f\"Labels contains -100: {(collated_batch['labels'] == -100).any().item()}\"\n",
    "        )\n",
    "\n",
    "        # Test forward pass with this batch\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                # Move to device\n",
    "                batch_device = {k: v.to(self.device) for k, v in collated_batch.items()}\n",
    "                outputs = self.model(**batch_device)\n",
    "                print(f\"Forward pass loss: {outputs.loss.item()}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Forward pass failed: {e}\")\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Train the model.\"\"\"\n",
    "        # Prepare dataset\n",
    "        datasets = self.prepare_training_data()\n",
    "\n",
    "        print(\"=== DEBUGGING BATCHES ===\")\n",
    "        self.debug_batch(datasets)\n",
    "        print(\"=== END DEBUGGING ===\")\n",
    "\n",
    "        # Training arguments\n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=str(self.output_dir),\n",
    "            # evaluation_strategy=\"epoch\",\n",
    "            eval_steps=100,\n",
    "            do_eval=True,\n",
    "            do_train=True,\n",
    "            save_steps=100,\n",
    "            eval_strategy=\"steps\",\n",
    "            save_strategy=\"steps\",\n",
    "            logging_strategy=\"steps\",\n",
    "            logging_steps=10,\n",
    "            learning_rate=self.learning_rate,\n",
    "            per_device_train_batch_size=self.batch_size,\n",
    "            per_device_eval_batch_size=self.batch_size,\n",
    "            num_train_epochs=self.num_epochs,\n",
    "            weight_decay=0.01,\n",
    "            save_total_limit=3,\n",
    "            predict_with_generate=False,\n",
    "            fp16=True,\n",
    "            logging_dir=str(self.output_dir / \"logs\"),\n",
    "            load_best_model_at_end=True,\n",
    "            log_level=\"info\",\n",
    "            report_to=\"none\",\n",
    "            gradient_accumulation_steps=1,\n",
    "            max_grad_norm=1.0,\n",
    "            warmup_steps=100,\n",
    "            dataloader_pin_memory=False,\n",
    "        )\n",
    "\n",
    "        # Data collator\n",
    "        data_collator = DataCollatorForSeq2Seq(\n",
    "            self.tokenizer, model=self.model, label_pad_token_id=-100\n",
    "        )\n",
    "\n",
    "        # Initialize trainer\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=datasets[\"train\"],\n",
    "            eval_dataset=datasets[\"validation\"],\n",
    "            tokenizer=self.tokenizer,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "\n",
    "        metrics = trainer.evaluate()\n",
    "        print(f\"Final validation loss: {metrics['eval_loss']:.4f}\")\n",
    "\n",
    "        # Save the model\n",
    "        trainer.save_model(str(self.output_dir))\n",
    "        self.tokenizer.save_pretrained(str(self.output_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T14:24:59.131725Z",
     "start_time": "2025-06-21T14:24:56.711466Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = FineTuningPipeline(\n",
    "    model_name=\"google/flan-t5-base\",\n",
    "    data_dir=\"/home/jiso/Documents/EPITA/action-learning/rag_medical/src/data/fine_tune_dataset/CancerQA.csv\",\n",
    "    output_dir=\"/home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5\",\n",
    "    batch_size=4,\n",
    "    num_epochs=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T14:27:02.426584Z",
     "start_time": "2025-06-21T14:25:00.462538Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "file_path: /home/jiso/Documents/EPITA/action-learning/rag_medical/src/data/fine_tune_dataset/CancerQA.csv\n",
      "                                            Question  \\\n",
      "0         What is (are) Non-Small Cell Lung Cancer ?   \n",
      "1   Who is at risk for Non-Small Cell Lung Cancer? ?   \n",
      "2  What are the symptoms of Non-Small Cell Lung C...   \n",
      "3       How to diagnose Non-Small Cell Lung Cancer ?   \n",
      "4  What is the outlook for Non-Small Cell Lung Ca...   \n",
      "\n",
      "                                              Answer   topic  split  \n",
      "0  Key Points\\n                    - Non-small ce...  cancer  train  \n",
      "1  Smoking is the major risk factor for non-small...  cancer  train  \n",
      "2  Signs of non-small cell lung cancer include a ...  cancer   test  \n",
      "3  Tests that examine the lungs are used to detec...  cancer  train  \n",
      "4  Certain factors affect prognosis (chance of re...  cancer  train  \n",
      "Preprocessing data...\n",
      "Creating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating embeddings: 100%|██████████| 683/683 [00:03<00:00, 194.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating training data: 100%|██████████| 683/683 [00:00<00:00, 19296.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample training data:\n",
      "Input: What is (are) Non-Small Cell Lung Cancer ?\n",
      "Output: ['What is (are) Small Cell Lung Cancer ?', 'What is (are) Renal Cell Cancer ?', 'What is (are) Hypopharyngeal Cancer ?']\n",
      "---\n",
      "Input: Who is at risk for Non-Small Cell Lung Cancer? ?\n",
      "Output: ['Who is at risk for Endometrial Cancer? ?', 'Who is at risk for Small Cell Lung Cancer? ?', 'Who is at risk for Adult Primary Liver Cancer? ?']\n",
      "---\n",
      "Input: What are the symptoms of Non-Small Cell Lung Cancer ?\n",
      "Output: ['What are the symptoms of Small Cell Lung Cancer ?', 'What are the symptoms of Endometrial Cancer ?', 'What are the symptoms of Pancreatic Cancer ?']\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 614/614 [00:00<00:00, 1242.29 examples/s]\n",
      "Map: 100%|██████████| 69/69 [00:00<00:00, 1233.67 examples/s]\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "/tmp/ipykernel_104592/2872925217.py:239: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "/home/jiso/anaconda3/envs/al/lib/python3.11/site-packages/accelerate/accelerator.py:449: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "Using auto half precision backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized data:\n",
      "Input IDs shape: 256\n",
      "Labels shape: 256\n",
      "First few input IDs: [2645, 19, 44, 1020, 21, 30588, 11759, 332, 13159, 9422]\n",
      "First few labels: [2645, 19, 44, 1020, 21, 12318, 11759, 332, 13159, 9422]\n",
      "Number of ignored tokens (-100): 126671/157184\n",
      "=== DEBUGGING BATCHES ===\n",
      "Raw batch structure:\n",
      "Item 0:\n",
      "  Input IDs length: 256\n",
      "  Labels length: 256\n",
      "  Input IDs sample: [2645, 19, 44, 1020, 21, 30588, 11759, 332, 13159, 9422]\n",
      "  Labels sample: [2645, 19, 44, 1020, 21, 12318, 11759, 332, 13159, 9422]\n",
      "  Labels has -100: True\n",
      "\n",
      "Item 1:\n",
      "  Input IDs length: 256\n",
      "  Labels length: 256\n",
      "  Input IDs sample: [363, 33, 8, 5872, 21, 30588, 8505, 2935, 7419, 5744]\n",
      "  Labels sample: [363, 33, 8, 5872, 21, 30588, 2808, 22450, 1162, 2149]\n",
      "  Labels has -100: True\n",
      "\n",
      "Collated batch:\n",
      "Input IDs shape: torch.Size([2, 256])\n",
      "Labels shape: torch.Size([2, 256])\n",
      "Labels contains -100: True\n",
      "Forward pass loss: 1.9573472738265991\n",
      "=== END DEBUGGING ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 614\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 462\n",
      "  Number of trainable parameters = 247,577,856\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='462' max='462' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [462/462 01:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 69\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-100\n",
      "Configuration saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-100/config.json\n",
      "Configuration saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-100/generation_config.json\n",
      "Model weights saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-100/model.safetensors\n",
      "tokenizer config file saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-100] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 69\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-200\n",
      "Configuration saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-200/config.json\n",
      "Configuration saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-200/generation_config.json\n",
      "Model weights saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-200/model.safetensors\n",
      "tokenizer config file saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-200] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 69\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-300\n",
      "Configuration saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-300/config.json\n",
      "Configuration saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-300/generation_config.json\n",
      "Model weights saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-300/model.safetensors\n",
      "tokenizer config file saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-300] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 69\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-400\n",
      "Configuration saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-400/config.json\n",
      "Configuration saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-400/generation_config.json\n",
      "Model weights saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-400/model.safetensors\n",
      "tokenizer config file saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-400] due to args.save_total_limit\n",
      "Saving model checkpoint to /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-462\n",
      "Configuration saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-462/config.json\n",
      "Configuration saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-462/generation_config.json\n",
      "Model weights saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-462/model.safetensors\n",
      "tokenizer config file saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-462/tokenizer_config.json\n",
      "Special tokens file saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/checkpoint-462/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 69\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5\n",
      "Configuration saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/config.json\n",
      "Configuration saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final validation loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/model.safetensors\n",
      "tokenizer config file saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/tokenizer_config.json\n",
      "Special tokens file saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/special_tokens_map.json\n",
      "tokenizer config file saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/tokenizer_config.json\n",
      "Special tokens file saved in /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "pipeline.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T14:43:43.705085Z",
     "start_time": "2025-06-21T14:43:43.048952Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question: What causes cancer?\n",
      "\n",
      "Recommended follow-up questions:\n",
      "1. What causes cancer? 2. What is the main cause? 3. What are the causes? 4. What does it mean to be a cancer patient?\n"
     ]
    }
   ],
   "source": [
    "def generate_recommendations(\n",
    "    model, tokenizer, question: str, max_length: int = 256\n",
    ") -> List[str]:\n",
    "    \"\"\"Generate question recommendations for a given medical question.\"\"\"\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        question,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    # Generate recommendations\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        num_beams=4,\n",
    "        temperature=0.7,\n",
    "        no_repeat_ngram_size=2,\n",
    "    )\n",
    "\n",
    "    # Decode and split recommendations\n",
    "    recommendations = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return recommendations.split(\" | \")\n",
    "\n",
    "\n",
    "question = \"What causes cancer?\"\n",
    "# Example usage\n",
    "prompt = (\n",
    "    \"Generate exactly four medically relevant follow-up questions based on the patient’s input question.\"\n",
    "    \"Each follow-up question must be concise, end with a question mark, and explore a different aspect of the topic \"\n",
    "    \"(e.g., diagnosis, treatment, risk factors, prognosis, prevention, or causes). \"\n",
    "    \"The follow-up questions must not repeat the patient’s question or use its exact wording. \"\n",
    "    \"Format the output as a numbered list (e.g., '1. Question?\\n2. Question?\\n3. Question?\\n4. Question?').\\n\\n\"\n",
    "    \"Example 1:\\n\"\n",
    "    'Patient: \"What are the symptoms of diabetes?\"\\n'\n",
    "    \"Follow-Up Questions:\\n\"\n",
    "    \"1. How is diabetes diagnosed?\\n\"\n",
    "    \"2. What are the treatment options for diabetes?\\n\"\n",
    "    \"3. Who is at risk for developing diabetes?\\n\"\n",
    "    \"4. What complications can arise from diabetes?\\n\\n\"\n",
    "    \"Example 2:\\n\"\n",
    "    'Patient: \"What is breast cancer?\"\\n'\n",
    "    \"Follow-Up Questions:\\n\"\n",
    "    \"1. What are the main risk factors for breast cancer?\\n\"\n",
    "    \"2. How is breast cancer diagnosed?\\n\"\n",
    "    \"3. What treatments are available for breast cancer?\\n\"\n",
    "    \"4. What is the prognosis for breast cancer patients?\\n\\n\"\n",
    "    f'Input Question: \"{question}\"\\n'\n",
    "    \"Follow-Up Questions:\".format(question)\n",
    ")\n",
    "recommendations = generate_recommendations(pipeline.model, pipeline.tokenizer, prompt)\n",
    "print(f\"Original question: {question}\")\n",
    "print(\"\\nRecommended follow-up questions:\")\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"{rec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T06:40:26.248808Z",
     "start_time": "2025-06-10T06:40:15.224040Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/config.json\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file /home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/flant5/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n",
      "Map: 100%|██████████| 68/68 [00:00<00:00, 817.40 examples/s]\n",
      "PyTorch: setting up devices\n",
      "/tmp/ipykernel_5016/3773851284.py:94: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "/home/jiso/anaconda3/envs/al/lib/python3.11/site-packages/accelerate/accelerator.py:449: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 68\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/jiso/.cache/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /home/jiso/.cache/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/vocab.json\n",
      "loading file merges.txt from cache at /home/jiso/.cache/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/merges.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/jiso/.cache/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/tokenizer_config.json\n",
      "loading file tokenizer.json from cache at /home/jiso/.cache/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/tokenizer.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at /home/jiso/.cache/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/jiso/.cache/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/jiso/.cache/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/model.safetensors\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': nan, 'eval_model_preparation_time': 0.0038, 'eval_exact_match': 0.0, 'eval_rougeL_f1': 0.1179912991081328, 'eval_bleu': 0.002034812795709879, 'eval_bertscore_f1': 0.8239430899129194, 'eval_runtime': 6.5132, 'eval_samples_per_second': 10.44, 'eval_steps_per_second': 1.382}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "# 1) Load metrics\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "bertscore_metric = evaluate.load(\"bertscore\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    # 2) Decode predictions and labels\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # 3) Exact-match\n",
    "    pred_lists = [p.split(\" | \") for p in decoded_preds]\n",
    "    label_lists = [l.split(\" | \") for l in decoded_labels]\n",
    "    em_scores = [\n",
    "        sum(1 for p in pred if p in gold) / max(len(gold), 1)\n",
    "        for pred, gold in zip(pred_lists, label_lists)\n",
    "    ]\n",
    "    exact_match = np.mean(em_scores)\n",
    "\n",
    "    # 4) ROUGE-L\n",
    "    rouge_preds = [\"\\n\".join(p) for p in pred_lists]\n",
    "    rouge_labels = [\"\\n\".join(l) for l in label_lists]\n",
    "    rouge_res = rouge_metric.compute(predictions=rouge_preds, references=rouge_labels)\n",
    "    rougeL_f1 = rouge_res[\"rougeL\"]  # already float\n",
    "\n",
    "    # 5) BLEU (string inputs)\n",
    "    # Provide list of hypothesis strings and list of reference strings\n",
    "    bleu_res = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    bleu_score = bleu_res[\"bleu\"]\n",
    "\n",
    "    # 6) BERTScore\n",
    "    bert_res = bertscore_metric.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, lang=\"en\"\n",
    "    )\n",
    "    bert_f1 = np.mean(bert_res[\"f1\"])\n",
    "\n",
    "    return {\n",
    "        \"exact_match\": exact_match,\n",
    "        \"rougeL_f1\": rougeL_f1,\n",
    "        \"bleu\": bleu_score,\n",
    "        \"bertscore_f1\": bert_f1,\n",
    "    }\n",
    "\n",
    "\n",
    "# 7) Load your fine-tuned model & tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(pipeline.output_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pipeline.output_dir)\n",
    "\n",
    "# 8) Rebuild your eval_dataset from the saved CSV\n",
    "df = pd.read_csv(f\"{pipeline.output_dir}/training_data.csv\")\n",
    "val_df = df.sample(frac=0.1, random_state=42)\n",
    "eval_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "\n",
    "def preprocess(ex):\n",
    "    enc = tokenizer(ex[\"input\"], truncation=True, padding=\"max_length\", max_length=256)\n",
    "    labs = tokenizer(\n",
    "        text_target=ex[\"follow_up_combined\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256,\n",
    "    )[\"input_ids\"]\n",
    "    # mask pads\n",
    "    labs = [lab if lab != tokenizer.pad_token_id else -100 for lab in labs]\n",
    "    enc[\"labels\"] = labs\n",
    "    return enc\n",
    "\n",
    "\n",
    "eval_dataset = eval_dataset.map(\n",
    "    preprocess, batched=False, remove_columns=eval_dataset.column_names\n",
    ")\n",
    "\n",
    "# 9) Create the Trainer and run evaluation\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=Seq2SeqTrainingArguments(\n",
    "        output_dir=str(pipeline.output_dir),\n",
    "        per_device_eval_batch_size=8,\n",
    "        predict_with_generate=True,\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(\n",
    "        tokenizer, model=model, label_pad_token_id=-100\n",
    "    ),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset)\n",
    "print(\"Evaluation results:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended follow-up questions:\n",
      "- What are the symptoms of diabetes? | What are the symptoms of diabetes? | What are the symptoms of diabetes?\n"
     ]
    }
   ],
   "source": [
    "model_path = \"google/flan-t5-base\"\n",
    "weights_path = \"/home/jiso/Documents/EPITA/action-learning/rag_medical/src/ml_models/model_files/flant5_diabetes.pth\"\n",
    "\n",
    "# Load model architecture\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "\n",
    "# Load saved weights\n",
    "state_dict = torch.load(\n",
    "    weights_path, map_location=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "def recommend_questions(input_text: str, max_length=128, num_return_sequences=1):\n",
    "    # Tokenize input\n",
    "    # prompt = f\"Input: {input_text}\\nInstruction: Recommend 3 follow-up medical questions.\"\n",
    "    prompt = f\"The response must be medically releavent question for the question: {input_text}\"\n",
    "\n",
    "    #     prompt = (\n",
    "    #     \"Generate exactly four medically relevant follow-up questions based on the patient’s input question.\"\n",
    "    #     \"Each follow-up question must be concise, end with a question mark, and explore a different aspect of the topic \"\n",
    "    #     \"(e.g., diagnosis, treatment, risk factors, prognosis, prevention, or causes). \"\n",
    "    #     \"The follow-up questions must not repeat the patient’s question or use its exact wording. \"\n",
    "    #     \"Format the output as a numbered list (e.g., '1. Question?\\n2. Question?\\n3. Question?\\n4. Question?').\\n\\n\"\n",
    "    #     \"Example 1:\\n\"\n",
    "    #     'Patient: \"What are the symptoms of diabetes?\"\\n'\n",
    "    #     \"Follow-Up Questions:\\n\"\n",
    "    #     \"1. How is diabetes diagnosed?\\n\"\n",
    "    #     \"2. What are the treatment options for diabetes?\\n\"\n",
    "    #     \"3. Who is at risk for developing diabetes?\\n\"\n",
    "    #     \"4. What complications can arise from diabetes?\\n\\n\"\n",
    "    #     \"Example 2:\\n\"\n",
    "    #     'Patient: \"What is breast cancer?\"\\n'\n",
    "    #     \"Follow-Up Questions:\\n\"\n",
    "    #     \"1. What are the main risk factors for breast cancer?\\n\"\n",
    "    #     \"2. How is breast cancer diagnosed?\\n\"\n",
    "    #     \"3. What treatments are available for breast cancer?\\n\"\n",
    "    #     \"4. What is the prognosis for breast cancer patients?\\n\\n\"\n",
    "    #     f'Input Question: \"{input_text}\"\\n'\n",
    "    #     \"Follow-Up Questions:\".format(input_text)\n",
    "    # )\n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_length,\n",
    "    ).to(device)\n",
    "\n",
    "    # Generate output\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            do_sample=False,  # change to True to sample diverse results\n",
    "        )\n",
    "\n",
    "    # Decode output\n",
    "    return [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "\n",
    "# input_q = \"Tell me about cancer?\"\n",
    "input_q = \"What are the symptoms of diabetes?\"\n",
    "recommendations = recommend_questions(input_q)\n",
    "\n",
    "print(\"Recommended follow-up questions:\")\n",
    "for q in recommendations:\n",
    "    print(\"-\", q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Question: When was a chemo therapy called chemotherapy?\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "def generate_question(topic: str) -> str:\n",
    "    prompt = f\"Generate one medical question about {topic}.\"\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=64\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=64,\n",
    "            num_return_sequences=1,\n",
    "            do_sample=True,  # 🔁 Enable randomness\n",
    "            top_k=50,  # Top-k sampling\n",
    "            top_p=0.9,  # Or use nucleus sampling\n",
    "            temperature=0.8,  # Add creativity\n",
    "        )\n",
    "\n",
    "    question = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return question.strip()\n",
    "\n",
    "\n",
    "topic = \"cancer\"\n",
    "question = generate_question(topic)\n",
    "print(\"Generated Question:\", question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deep-translator\n",
      "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /home/jiso/anaconda3/envs/al/lib/python3.11/site-packages (from deep-translator) (4.13.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /home/jiso/anaconda3/envs/al/lib/python3.11/site-packages (from deep-translator) (2.32.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/jiso/anaconda3/envs/al/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/jiso/anaconda3/envs/al/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (4.14.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/jiso/anaconda3/envs/al/lib/python3.11/site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jiso/anaconda3/envs/al/lib/python3.11/site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jiso/anaconda3/envs/al/lib/python3.11/site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jiso/anaconda3/envs/al/lib/python3.11/site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2025.7.14)\n",
      "Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
      "Installing collected packages: deep-translator\n",
      "Successfully installed deep-translator-1.11.4\n"
     ]
    }
   ],
   "source": [
    "!pip install deep-translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the cause of diabetes?\n",
      "Le calendrier pour recevoir un traitement contre le cancer peut varier en fonction de plusieurs facteurs, notamment:\n",
      "\n",
      "Diagnostic: Après un diagnostic de cancer, le traitement commence généralement en quelques semaines, selon le type et le stade du cancer.\n",
      "Plan de traitement: les oncologues élaboreront un plan de traitement personnalisé, qui peut inclure la chirurgie, la chimiothérapie, les radiations ou les thérapies ciblées.\n",
      "Assurance et logistique: la disponibilité des installations de traitement, des approbations d'assurance et de la planification peut affecter la date de début.\n",
      "Santé des patients: la santé globale et toute évaluation de prétraitement nécessaire peuvent également influencer le moment.\n",
      "Il est essentiel de discuter des délais spécifiques avec votre fournisseur de soins de santé pour les informations les plus précises.\n"
     ]
    }
   ],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "doc = \"\"\"The timeline for receiving cancer treatment can vary based on several factors, including:\n",
    "\n",
    "Diagnosis: After a cancer diagnosis, treatment typically begins within a few weeks, depending on the type and stage of cancer.\n",
    "Treatment plan: Oncologists will develop a personalized treatment plan, which may include surgery, chemotherapy, radiation, or targeted therapies.\n",
    "Insurance and logistics: Availability of treatment facilities, insurance approvals, and scheduling can affect the start date.\n",
    "Patient health: Overall health and any necessary pre-treatment evaluations can also influence timing.\n",
    "It's essential to discuss specific timelines with your healthcare provider for the most accurate information.\"\"\"\n",
    "\n",
    "# Translate to English\n",
    "text_en = GoogleTranslator(source=\"auto\", target=\"en\").translate(\n",
    "    \"Quelle est la cause du diabète?\"\n",
    ")\n",
    "print(text_en)  # \"What is the cause of diabetes?\"\n",
    "\n",
    "# Translate back to French\n",
    "response_fr = GoogleTranslator(source=\"en\", target=\"fr\").translate(doc)\n",
    "print(response_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03bfae2d52384bcaa525619caa3120b2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "08249caf995544ff8b6abbd00aca78f8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0d2eadfe6d1344478eee6c9dfc712aab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "11cf9f37c8d6466facaa35ce3afc8b2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7a605609a84e4dec9cb3527887c5c293",
      "placeholder": "​",
      "style": "IPY_MODEL_d326f4cd660d4b95a68b42bc7b364e19",
      "value": "Map: 100%"
     }
    },
    "27320cdc61cf4f378ab136ca2b897ec6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0d2eadfe6d1344478eee6c9dfc712aab",
      "max": 614,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e63ea984e33c4d5e954697e55646888b",
      "value": 614
     }
    },
    "2ee4740235ef4263bcbb11a4696f0f3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "62ef645e1e78488cba14f2795b5e4613": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "64405c41e83645d1b55bad5adf3ec39a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7a605609a84e4dec9cb3527887c5c293": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a54c23f55e0e4be497625b68a6805375": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "af773f2ccd6f430fa40ab175c96c9d37": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c01b308a72d849299858056cb495ccc0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_11cf9f37c8d6466facaa35ce3afc8b2d",
       "IPY_MODEL_27320cdc61cf4f378ab136ca2b897ec6",
       "IPY_MODEL_e5d592822cb3468aa8841362f2ca5fcf"
      ],
      "layout": "IPY_MODEL_03bfae2d52384bcaa525619caa3120b2"
     }
    },
    "c6c911728aba424c881eacaaf9429126": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce9250c66d6c4512a90f6ce945b61eb1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f64ab117b1294381aeba6457badea9c7",
      "placeholder": "​",
      "style": "IPY_MODEL_af773f2ccd6f430fa40ab175c96c9d37",
      "value": "Map: 100%"
     }
    },
    "d326f4cd660d4b95a68b42bc7b364e19": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d46f2bbb3da24973960a6641dc927c7d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_08249caf995544ff8b6abbd00aca78f8",
      "placeholder": "​",
      "style": "IPY_MODEL_a54c23f55e0e4be497625b68a6805375",
      "value": " 69/69 [00:00&lt;00:00, 1150.93 examples/s]"
     }
    },
    "e0061034ecce431cb21ddeb99c99f452": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c6c911728aba424c881eacaaf9429126",
      "max": 69,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2ee4740235ef4263bcbb11a4696f0f3b",
      "value": 69
     }
    },
    "e4f4e34a79f04ce9999622cfc0445b85": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ce9250c66d6c4512a90f6ce945b61eb1",
       "IPY_MODEL_e0061034ecce431cb21ddeb99c99f452",
       "IPY_MODEL_d46f2bbb3da24973960a6641dc927c7d"
      ],
      "layout": "IPY_MODEL_62ef645e1e78488cba14f2795b5e4613"
     }
    },
    "e5d592822cb3468aa8841362f2ca5fcf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f689bac961364670b99ec20f53e0cb83",
      "placeholder": "​",
      "style": "IPY_MODEL_64405c41e83645d1b55bad5adf3ec39a",
      "value": " 614/614 [00:00&lt;00:00, 3381.05 examples/s]"
     }
    },
    "e63ea984e33c4d5e954697e55646888b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f64ab117b1294381aeba6457badea9c7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f689bac961364670b99ec20f53e0cb83": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
